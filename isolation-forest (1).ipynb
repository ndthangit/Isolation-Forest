{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3632763,"sourceType":"datasetVersion","datasetId":2176461},{"sourceId":9871050,"sourceType":"datasetVersion","datasetId":6054170}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install python-docx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:11.698360Z","iopub.execute_input":"2025-01-08T12:49:11.698832Z","iopub.status.idle":"2025-01-08T12:49:24.307510Z","shell.execute_reply.started":"2025-01-08T12:49:11.698794Z","shell.execute_reply":"2025-01-08T12:49:24.306052Z"}},"outputs":[{"name":"stdout","text":"Collecting python-docx\n  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from python-docx) (5.3.0)\nRequirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.10/site-packages (from python-docx) (4.12.2)\nDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: python-docx\nSuccessfully installed python-docx-1.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install Document","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:24.310035Z","iopub.execute_input":"2025-01-08T12:49:24.310440Z","iopub.status.idle":"2025-01-08T12:49:37.350531Z","shell.execute_reply.started":"2025-01-08T12:49:24.310403Z","shell.execute_reply":"2025-01-08T12:49:37.349172Z"}},"outputs":[{"name":"stdout","text":"Collecting Document\n  Downloading document-1.0.zip (12 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: Document\n  Building wheel for Document (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for Document: filename=document-1.0-py3-none-any.whl size=8764 sha256=ec340bf745250d3cdb3dc49ba59250a7032265d00e6c8e25c6c41a85a4dae6e8\n  Stored in directory: /root/.cache/pip/wheels/68/ac/8e/4bf7a4f454fcbd96af3be9f23268ee72151302643d5b8d5290\nSuccessfully built Document\nInstalling collected packages: Document\nSuccessfully installed Document-1.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install polars pyarrow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:37.352333Z","iopub.execute_input":"2025-01-08T12:49:37.352760Z","iopub.status.idle":"2025-01-08T12:49:47.866109Z","shell.execute_reply.started":"2025-01-08T12:49:37.352721Z","shell.execute_reply":"2025-01-08T12:49:47.864816Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: polars in /opt/conda/lib/python3.10/site-packages (1.9.0)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (17.0.0)\nRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.26.4)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom docx import Document\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix,precision_score, make_scorer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:47.869291Z","iopub.execute_input":"2025-01-08T12:49:47.869688Z","iopub.status.idle":"2025-01-08T12:49:47.995829Z","shell.execute_reply.started":"2025-01-08T12:49:47.869649Z","shell.execute_reply":"2025-01-08T12:49:47.994642Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import h2o\nfrom h2o.estimators import H2OExtendedIsolationForestEstimator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:47.997474Z","iopub.execute_input":"2025-01-08T12:49:47.998444Z","iopub.status.idle":"2025-01-08T12:49:48.424857Z","shell.execute_reply.started":"2025-01-08T12:49:47.998403Z","shell.execute_reply":"2025-01-08T12:49:48.423592Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nh2o.init()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:48.426383Z","iopub.execute_input":"2025-01-08T12:49:48.426745Z","iopub.status.idle":"2025-01-08T12:49:57.529738Z","shell.execute_reply.started":"2025-01-08T12:49:48.426709Z","shell.execute_reply":"2025-01-08T12:49:57.528572Z"}},"outputs":[{"name":"stdout","text":"Checking whether there is an H2O instance running at http://localhost:54321..... not found.\nAttempting to start a local H2O server...\n  Java Version: openjdk version \"11.0.24\" 2024-07-16; OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu320.04); OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu320.04, mixed mode, sharing)\n  Starting server from /opt/conda/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n  Ice root: /tmp/tmp3_mtp1z_\n  JVM stdout: /tmp/tmp3_mtp1z_/h2o_unknownUser_started_from_python.out\n  JVM stderr: /tmp/tmp3_mtp1z_/h2o_unknownUser_started_from_python.err\n  Server is running at http://127.0.0.1:54321\nConnecting to H2O server at http://127.0.0.1:54321 ... successful.\nWarning: Your H2O cluster version is (4 months and 10 days) old.  There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"--------------------------  ----------------------------------\nH2O_cluster_uptime:         03 secs\nH2O_cluster_timezone:       Etc/UTC\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.46.0.5\nH2O_cluster_version_age:    4 months and 10 days\nH2O_cluster_name:           H2O_from_python_unknownUser_8r0q9r\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    7.500 Gb\nH2O_cluster_total_cores:    4\nH2O_cluster_allowed_cores:  4\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://127.0.0.1:54321\nH2O_connection_proxy:       {\"http\": null, \"https\": null}\nH2O_internal_security:      False\nPython_version:             3.10.14 final\n--------------------------  ----------------------------------","text/html":"\n<style>\n\n#h2o-table-1.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-1 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-1 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-1 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-1 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-1 .h2o-table th,\n#h2o-table-1 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-1 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-1\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>03 secs</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Etc/UTC</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.46.0.5</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>4 months and 10 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>H2O_from_python_unknownUser_8r0q9r</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>7.500 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>4</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>4</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://127.0.0.1:54321</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>{\"http\": null, \"https\": null}</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.10.14 final</td></tr></tbody>\n  </table>\n</div>\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# print(h2o_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.531308Z","iopub.execute_input":"2025-01-08T12:49:57.531769Z","iopub.status.idle":"2025-01-08T12:49:57.538345Z","shell.execute_reply.started":"2025-01-08T12:49:57.531722Z","shell.execute_reply":"2025-01-08T12:49:57.536776Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# print(eif_result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.539573Z","iopub.execute_input":"2025-01-08T12:49:57.540004Z","iopub.status.idle":"2025-01-08T12:49:57.552156Z","shell.execute_reply.started":"2025-01-08T12:49:57.539956Z","shell.execute_reply":"2025-01-08T12:49:57.550927Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# import tensorflow as tf\n# device_name = tf.test.gpu_device_name()\n# if device_name != '/device:GPU:0':\n#   raise SystemError('GPU device not found')\n# print('Found GPU at: {}'.format(device_name))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.553799Z","iopub.execute_input":"2025-01-08T12:49:57.557029Z","iopub.status.idle":"2025-01-08T12:49:57.565673Z","shell.execute_reply.started":"2025-01-08T12:49:57.556972Z","shell.execute_reply":"2025-01-08T12:49:57.564674Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"chunksize = 2*(10**5)  # Số dòng mỗi chunk (tùy chỉnh theo RAM)\nfilename = '/kaggle/input/nfuqnidsv2-network-intrusion-detection-dataset/NF-UQ-NIDS-v2.csv'\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.570681Z","iopub.execute_input":"2025-01-08T12:49:57.571199Z","iopub.status.idle":"2025-01-08T12:49:57.580956Z","shell.execute_reply.started":"2025-01-08T12:49:57.571133Z","shell.execute_reply":"2025-01-08T12:49:57.579736Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Lưu từng chunk vào danh sách hoặc xử lý ngay\n# train_url= pd.read_csv(filename, chunksize=chunksize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.582535Z","iopub.execute_input":"2025-01-08T12:49:57.582981Z","iopub.status.idle":"2025-01-08T12:49:57.594639Z","shell.execute_reply.started":"2025-01-08T12:49:57.582929Z","shell.execute_reply":"2025-01-08T12:49:57.593396Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# dem  =0\n# for i in train_url:\n#     dem = dem+1\n# print(dem)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.596165Z","iopub.execute_input":"2025-01-08T12:49:57.596590Z","iopub.status.idle":"2025-01-08T12:49:57.611739Z","shell.execute_reply.started":"2025-01-08T12:49:57.596535Z","shell.execute_reply":"2025-01-08T12:49:57.610682Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Define the mapping of attack types to integer keys\nattack_type_map = {\n    'Benign': 0,\n    'DoS': 1,\n    'scanning': 2,\n    'DDoS': 3,\n    'xss': 4,\n    'Bot': 5,\n    'Reconnaissance': 6,\n    'password': 7,\n    'Fuzzers': 8,\n    'injection': 9,\n    'Theft': 10,\n    'Brute Force': 11,\n    'Infilteration': 12,\n    'Exploits': 13,\n    'Generic': 14,\n    'Analysis': 15,\n    'Backdoor': 16,\n    'mitm': 17,\n    'Shellcode': 18,\n    'ransomware': 19,\n    'Worms': 20\n}\n\ndrop_column={\n    'Dataset',\n    'IPV4_SRC_ADDR',\n    'IPV4_DST_ADDR',\n    'L7_PROTO',\n    'SRC_TO_DST_SECOND_BYTES',\n    'DST_TO_SRC_SECOND_BYTES',\n    'FTP_COMMAND_RET_CODE',\n\n}\ndrop_purposed_column= {\n    'Label',\n    'Attack'\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.614471Z","iopub.execute_input":"2025-01-08T12:49:57.614968Z","iopub.status.idle":"2025-01-08T12:49:57.625683Z","shell.execute_reply.started":"2025-01-08T12:49:57.614894Z","shell.execute_reply":"2025-01-08T12:49:57.624527Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# total_counts = Counter()\n\n# label_count= Counter()\n# for i in range(1,91):\n#     data = pd.read_csv(f'/kaggle/input/nf-uq-nids-v2/part_{i}.csv')\n#     num_type = Counter(data['Attack'])\n#     label_type= Counter(data['Label'])\n#     total_counts.update(num_type)\n#     label_count.update(label_type)\n# print(\"\\nTổng số lượng của mỗi phần tử trong tất cả các file:\", total_counts)\n# print(\"\\nTổng số lượng của mỗi nhãn trong tất cả các file:\", label_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.627457Z","iopub.execute_input":"2025-01-08T12:49:57.628402Z","iopub.status.idle":"2025-01-08T12:49:57.638592Z","shell.execute_reply.started":"2025-01-08T12:49:57.628278Z","shell.execute_reply":"2025-01-08T12:49:57.637317Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# scaler = StandardScaler()\n# data_normalized = scaler.fit_transform(dataset)\n# print(data_normalized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.641527Z","iopub.execute_input":"2025-01-08T12:49:57.641994Z","iopub.status.idle":"2025-01-08T12:49:57.657510Z","shell.execute_reply.started":"2025-01-08T12:49:57.641943Z","shell.execute_reply":"2025-01-08T12:49:57.656247Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# plt.figure(figsize=(20,10))\n# plt.bar(total_counts.keys(), total_counts.values())\n# plt.savefig('/kaggle/working/total_counts.png')\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.659752Z","iopub.execute_input":"2025-01-08T12:49:57.660247Z","iopub.status.idle":"2025-01-08T12:49:57.670076Z","shell.execute_reply.started":"2025-01-08T12:49:57.660191Z","shell.execute_reply":"2025-01-08T12:49:57.668853Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# plt.figure(figsize=(20,10))\n# plt.bar(label_count.keys(), label_count.values())\n# plt.savefig('/kaggle/working/label_count.png')\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.672186Z","iopub.execute_input":"2025-01-08T12:49:57.672823Z","iopub.status.idle":"2025-01-08T12:49:57.684892Z","shell.execute_reply.started":"2025-01-08T12:49:57.672768Z","shell.execute_reply":"2025-01-08T12:49:57.683475Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# train_url =[f'/kaggle/input/nf-uq-nids-v2/part_{i}.csv' for i in range(training_start,training_end)]\n# train_url =['/kaggle/input/nfuqnidsv2-network-intrusion-detection-dataset/NF-UQ-NIDS-v2.csv']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.686221Z","iopub.execute_input":"2025-01-08T12:49:57.686662Z","iopub.status.idle":"2025-01-08T12:49:57.697089Z","shell.execute_reply.started":"2025-01-08T12:49:57.686605Z","shell.execute_reply":"2025-01-08T12:49:57.695788Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def pre_processing(data):\n    data_copy = data.copy()\n    data_copy.drop(columns=drop_column, inplace=True)\n    data_copy.drop(columns=drop_purposed_column, inplace=True)\n    return data_copy\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.698898Z","iopub.execute_input":"2025-01-08T12:49:57.699392Z","iopub.status.idle":"2025-01-08T12:49:57.711096Z","shell.execute_reply.started":"2025-01-08T12:49:57.699337Z","shell.execute_reply":"2025-01-08T12:49:57.709667Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# def train_model(processing_function):\n#     prediction =[]\n#     original_result=[]\n    \n#     model = IsolationForest()\n    \n#     for data in train_url:\n#         # data = pd.read_csv(train_path)\n#         label = data['Label']\n#         label.replace(0, -1, inplace=True)\n    \n#         attack = data['Attack']\n#         data = processing_function(data)  \n#         scaler = StandardScaler()\n#         data_normalized = scaler.fit_transform(data)\n#         model.fit(data_normalized)\n        \n#         y_pred = model.predict(data_normalized)\n        \n#         prediction.append(y_pred)\n#         original_result.append(label)\n#     #convert to 1D array\n#     prediction = np.concatenate(prediction)\n#     original_result = np.concatenate(original_result)\n#     acc = accuracy_score(original_result, prediction)\n#     f1 = f1_score(original_result, prediction)\n#     roc_auc = roc_auc_score(original_result, prediction)\n#     cm = confusion_matrix(original_result, prediction)\n    \n#     return acc, f1,roc_auc,cm\n# def train_model(processing_function):\n#     prediction = []\n#     original_result = []\n    \n#     for data in train_url:\n#         # data = pd.read_csv(train_path)\n#         label = data['Label']\n#         label.replace(0, -1, inplace=True)\n        \n#         # Tính tỷ lệ contamination dựa trên nhãn label\n#         contamination = (label == -1).mean()  # Tỷ lệ các giá trị -1\n        \n#         attack = data['Attack']\n#         data = processing_function(data)  \n#         scaler = StandardScaler()\n#         data_normalized = scaler.fit_transform(data)\n        \n#         # Khởi tạo IsolationForest với contamination động\n#         model = IsolationForest(contamination=contamination)\n#         model.fit(data_normalized)\n        \n#         y_pred = model.predict(data_normalized)\n        \n#         prediction.append(y_pred)\n#         original_result.append(label)\n    \n#     # Convert to 1D array\n#     prediction = np.concatenate(prediction)\n#     original_result = np.concatenate(original_result)\n#     acc = accuracy_score(original_result, prediction)\n#     f1 = f1_score(original_result, prediction)\n#     roc_auc = roc_auc_score(original_result, prediction)\n#     cm = confusion_matrix(original_result, prediction)\n    \n#     return acc, f1, roc_auc, cm\n\ndef train_model(processing_function):\n    prediction = []\n    original_result = []\n\n    count =0\n\n    for data in pd.read_csv(filename, chunksize=chunksize):  # Đảm bảo train_data là danh sách các DataFrame\n        # Tạo bản sao của dữ liệu để bảo vệ dữ liệu gốc\n        if count >10:\n            break\n        else:\n            count+=1\n        # Xử lý cột Label\n        label = data['Label']\n        label.replace(0, -1, inplace=True)\n\n        # Tính tỷ lệ contamination dựa trên nhãn label\n        contamination = (label == -1).mean()  # Tỷ lệ các giá trị -1\n\n        # Xử lý dữ liệu\n        data_processed = processing_function(data)\n        scaler = StandardScaler()\n        data_normalized = scaler.fit_transform(data_processed)\n\n        # Chuyển dữ liệu thành H2OFrame\n        h2o_data = h2o.H2OFrame(data_normalized)\n\n        # Thiết lập giá trị sample_size hợp lệ\n        # sample_size = min(len(data_processed), 10000)  # Giới hạn trong khoảng [2, 100000]\n        # if sample_size < 2:\n        #     sample_size = 2  # Đảm bảo sample_size ít nhất là 2\n\n        # Khởi tạo H2OExtendedIsolationForestEstimator\n        model = H2OExtendedIsolationForestEstimator(\n            model_id=\"eif.hex\",\n            extension_level=15,  # Đảm bảo giá trị hợp lệ\n            sample_size=10000  # Thiết lập sample_size\n        )\n        model.train(training_frame=h2o_data)\n\n        # Dự đoán\n        h2o_pred = model.predict(h2o_data)\n\n        # Sử dụng đa luồng nếu cài đặt polars và pyarrow\n        try:\n            h2o_pred_df = h2o_pred.as_data_frame(use_multi_thread=True).iloc[:, 0].values\n        except TypeError:\n            warnings.warn(\n                \"Using single-thread for converting H2O frame to pandas dataframe. \"\n                \"Install 'polars' and 'pyarrow' for faster multi-thread conversion.\"\n            )\n            h2o_pred_df = h2o_pred.as_data_frame()\n\n        threshold = np.percentile(h2o_pred_df, 80)\n\n        # Phân loại dựa trên ngưỡng\n        y_pred = np.where(h2o_pred_df > threshold, -1, 1)\n\n        prediction.append(y_pred)\n        original_result.append(label.values)  # Chuyển label thành mảng numpy\n\n    # Convert to 1D array\n    prediction = np.concatenate(prediction)\n    original_result = np.concatenate(original_result)\n    acc = accuracy_score(original_result, prediction)\n    f1 = f1_score(original_result, prediction)\n    roc_auc = roc_auc_score(original_result, prediction)\n    cm = confusion_matrix(original_result, prediction)\n    print(acc, f1, roc_auc, cm)\n    return acc, f1, roc_auc, cm\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.712583Z","iopub.execute_input":"2025-01-08T12:49:57.713061Z","iopub.status.idle":"2025-01-08T12:49:57.734202Z","shell.execute_reply.started":"2025-01-08T12:49:57.713009Z","shell.execute_reply":"2025-01-08T12:49:57.731738Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"\ndef train_model_if(processing_function):\n    prediction = []\n    original_result = []\n\n    count = 0\n\n    for data in pd.read_csv(filename, chunksize=chunksize):  # Load data in chunks\n        if count > 10:  # Limit to 20 chunks\n            break\n        else:\n            count += 1\n        \n        # Process label column\n        label = data['Label']\n        label.replace(0, -1, inplace=True)  # Replace 0 with -1 (anomaly)\n\n        # Calculate contamination ratio\n        contamination = (label == -1).mean()\n\n        # Process data using provided function\n        data_processed = processing_function(data)\n        \n        # Normalize the data\n        scaler = StandardScaler()\n        data_normalized = scaler.fit_transform(data_processed)\n\n        # Initialize Scikit-learn's Isolation Forest\n        model = IsolationForest(\n            n_estimators=100,\n            contamination=contamination,\n            random_state=42,\n        )\n        model.fit(data_normalized)\n\n        # Predict anomaly scores\n        anomaly_scores = model.decision_function(data_normalized)\n        threshold = np.percentile(anomaly_scores, 80)  # Top 20% anomalies as threshold\n\n        # Classify based on threshold\n        y_pred = np.where(anomaly_scores > threshold, -1, 1)\n\n        # Store predictions and original labels\n        prediction.append(y_pred)\n        original_result.append(label.values)\n\n    # Convert lists to arrays\n    prediction = np.concatenate(prediction)\n    original_result = np.concatenate(original_result)\n\n    # Calculate evaluation metrics\n    acc = accuracy_score(original_result, prediction)\n    f1 = f1_score(original_result, prediction)\n    roc_auc = roc_auc_score(original_result, prediction)\n    cm = confusion_matrix(original_result, prediction)\n\n    print(f\"Accuracy: {acc}, F1 Score: {f1}, ROC AUC: {roc_auc}\")\n    print(f\"Confusion Matrix:\\n{cm}\")\n\n    return acc, f1, roc_auc, cm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:59:29.473311Z","iopub.execute_input":"2025-01-08T12:59:29.473855Z","iopub.status.idle":"2025-01-08T12:59:29.484349Z","shell.execute_reply.started":"2025-01-08T12:59:29.473806Z","shell.execute_reply":"2025-01-08T12:59:29.483166Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def train_model_with_threshold(processing_function, train_data, contamination_rate=0.05):\n    prediction = []\n    original_result = []\n    count =0\n    \n    model = H2OExtendedIsolationForestEstimator(\n            model_id=\"eif.hex\",\n            extension_level=30,\n            sample_size=10000\n        )\n\n    for data in pd.read_csv(filename, chunksize=chunksize):\n        # Xử lý nhãn\n        if count >10:\n            break\n        else:\n            count+=1\n            \n        label = data['Label'].copy()  # Sao chép nhãn để đảm bảo tính bất biến\n        label.replace(0, -1, inplace=True)\n\n        # Xử lý dữ liệu\n        data_processed = processing_function(data)\n        scaler = StandardScaler()\n        data_normalized = scaler.fit_transform(data_processed)\n        h2o_data = h2o.H2OFrame(data_normalized)\n\n        # Khởi tạo và huấn luyện mô hình\n        \n        model.train(training_frame=h2o_data)\n\n        # Dự đoán anomaly score\n        h2o_pred = model.predict(h2o_data)\n        try:\n            h2o_pred_df = h2o_pred.as_data_frame(use_multi_thread=True).iloc[:, 0].values\n        except TypeError:\n            warnings.warn(\n                \"Using single-thread for converting H2O frame to pandas dataframe. \"\n                \"Install 'polars' and 'pyarrow' for faster multi-thread conversion.\"\n            )\n            h2o_pred_df = h2o_pred.as_data_frame()\n\n        # Tìm ngưỡng dựa trên contamination_rate\n        threshold = np.percentile(h2o_pred_df, (1 - contamination_rate) * 100)\n\n        # Phân loại dựa trên ngưỡng\n        y_pred = np.where(h2o_pred_df > threshold, -1, 1)\n\n        prediction.append(y_pred)\n        original_result.append(label.values)\n\n    # Kết hợp kết quả\n    prediction = np.concatenate(prediction)\n    original_result = np.concatenate(original_result)\n\n    # Tính các metric\n    acc = accuracy_score(original_result, prediction)\n    f1 = f1_score(original_result, prediction)\n    roc_auc = roc_auc_score(original_result, prediction)\n    cm = confusion_matrix(original_result, prediction)\n    print(acc, f1, roc_auc, cm)\n    return acc, f1, roc_auc, cm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.762449Z","iopub.execute_input":"2025-01-08T12:49:57.762976Z","iopub.status.idle":"2025-01-08T12:49:57.783988Z","shell.execute_reply.started":"2025-01-08T12:49:57.762900Z","shell.execute_reply":"2025-01-08T12:49:57.782718Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def plot_metrics(acc, f1, roc_auc, save_path='/kaggle/working/no_grid.png'):\n    \"\"\"\n    Hàm vẽ biểu đồ các chỉ số đánh giá và lưu ảnh biểu đồ.\n\n    Parameters:\n    - acc (float): Giá trị Accuracy.\n    - f1 (float): Giá trị F1-Score.\n    - roc_auc (float): Giá trị ROC AUC.\n    - save_path (str): Đường dẫn để lưu biểu đồ (mặc định: '/kaggle/working/no_grid.png').\n\n    Returns:\n    - None\n    \"\"\"\n    metrics = ['Accuracy', 'F1-Score', 'ROC AUC']\n    values = [acc, f1, roc_auc]\n\n    # Vẽ biểu đồ\n    plt.figure(figsize=(8, 6))\n    plt.bar(metrics, values, color=['blue', 'green', 'orange'])\n    plt.ylim(0, 1)  # Giới hạn trục y từ 0 đến 1\n    plt.title('Evaluation Metrics')\n    plt.ylabel('Score')\n    plt.xlabel('Metrics')\n\n    # Hiển thị giá trị trên cột\n    for i, v in enumerate(values):\n        plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontsize=12)\n    \n    # Lưu biểu đồ và hiển thị\n    plt.savefig(save_path)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.785335Z","iopub.execute_input":"2025-01-08T12:49:57.785710Z","iopub.status.idle":"2025-01-08T12:49:57.803526Z","shell.execute_reply.started":"2025-01-08T12:49:57.785675Z","shell.execute_reply":"2025-01-08T12:49:57.802295Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def save_results_to_docx(filename, results):\n    \"\"\"\n    Lưu kết quả vào tệp .docx.\n    \n    :param filename: Tên tệp .docx để lưu kết quả\n    :param results: Danh sách chứa các kết quả\n    \"\"\"\n    doc = Document()\n    doc.add_heading(\"Results from train_model_with_threshold\", level=1)\n\n    for result in results:\n        doc.add_paragraph(result)\n\n    # Lưu tệp\n    doc.save(filename)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.805226Z","iopub.execute_input":"2025-01-08T12:49:57.805604Z","iopub.status.idle":"2025-01-08T12:49:57.818526Z","shell.execute_reply.started":"2025-01-08T12:49:57.805569Z","shell.execute_reply":"2025-01-08T12:49:57.817421Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# # Thực hiện các lần chạy và lưu kết quả\n# results = []  # Danh sách lưu kết quả từng lần chạy\n\n# # Các mức contamination_rate cần kiểm tra\n# contamination_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n\n# # Chạy hàm với từng contamination_rate\n# for rate in contamination_rates:\n#     print(f\"Running train_model_with_threshold with contamination_rate={rate}\")\n    \n#     # Thay thế hàm bên dưới bằng thực tế của bạn\n#     acc, f1, roc_auc, cm = train_model_with_threshold(\n#         processing_function=pre_processing,\n#         train_data= train_url,\n#         contamination_rate=rate\n#     )\n    \n#     # Định dạng kết quả\n#     result_text = f\"Contamination Rate: {rate}\\n\" \\\n#                   f\"Accuracy: {acc:.4f}\\n\" \\\n#                   f\"F1 Score: {f1:.4f}\\n\" \\\n#                   f\"ROC AUC: {roc_auc:.4f}\\n\" \\\n#                   f\"Confusion Matrix: \\n{cm}\\n\"\n    \n#     # Thêm kết quả vào danh sách\n#     results.append(result_text)\n\n# # Lưu tất cả kết quả vào tệp .docx\n# output_filename = \"train_model_results.docx\"\n# save_results_to_docx(output_filename, results)\n\n# print(f\"Results saved to {output_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.820223Z","iopub.execute_input":"2025-01-08T12:49:57.820623Z","iopub.status.idle":"2025-01-08T12:49:57.833360Z","shell.execute_reply.started":"2025-01-08T12:49:57.820585Z","shell.execute_reply":"2025-01-08T12:49:57.832008Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Vẽ ma trận nhầm lẫn\n\ndef plot_confusion_matrix(cm, save_path='/kaggle/working/confusion_matrix_no_grid.png'):\n    \"\"\"\n    Plots and saves a confusion matrix.\n\n    Parameters:\n        cm (array-like): Confusion matrix data.\n        save_path (str): Path to save the plot image. Default is '/kaggle/working/confusion_matrix_no_grid.png'.\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.savefig(save_path)\n    plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.835533Z","iopub.execute_input":"2025-01-08T12:49:57.836020Z","iopub.status.idle":"2025-01-08T12:49:57.854172Z","shell.execute_reply.started":"2025-01-08T12:49:57.835969Z","shell.execute_reply":"2025-01-08T12:49:57.853005Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# print(train_model(processing_function=pre_processing))\nacc0,f10,auc0,cm0=train_model(processing_function=pre_processing)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:17:06.664713Z","iopub.execute_input":"2025-01-08T08:17:06.665239Z"}},"outputs":[{"name":"stdout","text":"Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\nextendedisolationforest Model Build progress: |██████████████████████████████████| (done) 100%\nextendedisolationforest prediction progress: |███████████████████████████████████| (done) 100%\nExport File progress: |██████████████████████████████████████████████████████████| (done) 100%\nParse progress: |████████████████████████████████████████████████████████████████| (done) 100%\nextendedisolationforest Model Build progress: |██████████████████████████████████| (done) 100%\nextendedisolationforest prediction progress: |███████████████████████████████████| (done) 100%\nExport File progress: |██████████████████████████████████████████████████████████| (done) 100%\nParse progress: |████████████████████████████████████████████████████████████████| (done) 100%\nextendedisolationforest Model Build progress: |██████████████████████████████████| (done) 100%\nextendedisolationforest prediction progress: |███████████████████████████████████| (done) 100%\nExport File progress: |██████████████████████████████████████████████████████████| (done) 100%\nParse progress: |████████████████████████████████████████████████████████████████| (done) 100%\nextendedisolationforest Model Build progress: |██████████████████████████████████| (done) 100%\nextendedisolationforest prediction progress: |███████████████████████████████████| (done) 100%\nExport File progress: |██████████████████████████████████████████████████████████| (done) 100%\nParse progress: |████████████████████████████████████████████████████████████████| (done) 100%\nextendedisolationforest Model Build progress: |██████████████████████████████████| (done) 100%\nextendedisolationforest prediction progress: |██","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plot_metrics(acc=acc0, f1=f10, roc_auc=auc0,save_path='/kaggle/working/no_eif.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:49:57.856148Z","iopub.execute_input":"2025-01-08T12:49:57.856582Z","iopub.status.idle":"2025-01-08T12:49:57.897752Z","shell.execute_reply.started":"2025-01-08T12:49:57.856475Z","shell.execute_reply":"2025-01-08T12:49:57.896294Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_metrics(acc\u001b[38;5;241m=\u001b[39m\u001b[43macc0\u001b[49m, f1\u001b[38;5;241m=\u001b[39mf10, roc_auc\u001b[38;5;241m=\u001b[39mauc0,save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/no_eif.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'acc0' is not defined"],"ename":"NameError","evalue":"name 'acc0' is not defined","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"plot_confusion_matrix(cm0,save_path='/kaggle/working/confusion_matrix_no_eif.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc0if,f10if,auc0if,cm0if=train_model_if(processing_function=pre_processing)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:59:34.374350Z","iopub.execute_input":"2025-01-08T12:59:34.374801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(acc=acc0if, f1=f10if, roc_auc=auc0if,save_path='/kaggle/working/no_if.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(cm0if,save_path='/kaggle/working/confusion_matrix_no_if.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install \\\n#   --extra-index-url=https://pypi.nvidia.com \\\n#   cudf-cu12==24.12.* \\\n#   dask-cudf-cu12==24.12.* \\\n#   cuml-cu12==24.12.* \\\n#   cugraph-cu12==24.12.*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\ndef pre_processing_if_PCA(data):\n    if isinstance(data, np.ndarray):\n        data = pd.DataFrame(data)\n\n    # Tạo bản sao của dữ liệu để không làm thay đổi data gốc\n    data_copy = data.copy()\n\n    # Xóa các cột không cần thiết từ bản sao\n    data_copy.drop(columns=drop_column, inplace=True)\n    data_copy.drop(columns=drop_purposed_column, inplace=True)\n\n    pca = PCA(n_components=0.5)  \n    data_reduced = pca.fit_transform(data_copy)    \n    \n    return data_reduced\n\nfrom h2o.estimators import H2OPrincipalComponentAnalysisEstimator\n\ndef pre_processing_PCA(data):\n    # Chuyển đổi dữ liệu sang pandas DataFrame nếu cần\n    if isinstance(data, np.ndarray):\n        data = pd.DataFrame(data)\n\n    # Tạo bản sao của dữ liệu để không làm thay đổi data gốc\n    data_copy = data.copy()\n\n    # Xóa các cột không cần thiết từ bản sao\n    data_copy.drop(columns=drop_column, inplace=True)\n    data_copy.drop(columns=drop_purposed_column, inplace=True)\n    \n    # Chuyển đổi dữ liệu sang H2OFrame\n    h2o_data = h2o.H2OFrame(data_copy)\n\n    # Áp dụng PCA bằng H2O\n    pca_model = H2OPrincipalComponentAnalysisEstimator(\n        pca_method= \"glrm\",  #\"GramSVD\"  # Hoặc Power\n        k=30,\n        transform=\"STANDARDIZE\",\n        impute_missing=True,\n        use_all_factor_levels=True\n    )\n    pca_model.train(training_frame=h2o_data)\n\n    # Tính tỷ lệ phương sai tích lũy từ bảng importance\n    # importance = pca_model._model_json[\"output\"][\"importance\"]\n    # var_exp_ratio = np.cumsum(importance[1])  # `importance[1]` chứa tỷ lệ phương sai giải thích\n\n    # # Xác định số lượng thành phần cần giữ (>= 95% phương sai)\n    # n_components = int(np.argmax(var_exp_ratio >= 0.99) + 1)  # Chuyển đổi sang kiểu int\n\n    # print(n_components)\n\n    # # Huấn luyện lại PCA với số lượng thành phần đã chọn\n    # pca_model.k = n_components\n    # pca_model.train(training_frame=h2o_data)\n\n    # Dự đoán (trích xuất dữ liệu PCA)\n    reduced_data = pca_model.predict(h2o_data).as_data_frame(use_multi_thread=True).values\n    \n    return reduced_data\n\n\n# def pre_processing_PCA(data):\n#     # Xóa các cột không cần thiết\n#     data.drop(columns=drop_column, inplace=True)\n#     data.drop(columns=drop_purposed_column, inplace=True)\n    \n#     # Chuyển đổi dữ liệu thành mảng NumPy để sử dụng với cuML\n#     data_array = data.values.astype(np.float32)\n    \n#     # Áp dụng PCA bằng cuML\n#     pca_model = PCA(n_components=data.shape[1])\n#     transformed_data = pca_model.fit_transform(data_array)\n    \n#     # Tính tỷ lệ phương sai tích lũy\n#     explained_variance_ratio = np.cumsum(pca_model.explained_variance_ratio_)\n    \n#     # Xác định số lượng thành phần cần giữ (>= threshold phương sai)\n#     n_components = np.argmax(explained_variance_ratio >= 0.6) + 1\n    \n#     # Huấn luyện lại PCA với số lượng thành phần đã chọn\n#     pca_model = PCA(n_components=n_components)\n#     reduced_data = pca_model.fit_transform(data_array)\n    \n#     return reduced_data\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/nf-uq-nids-v2/part_1.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = pre_processing_PCA(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from cuml.decomposition import PCA \n# from sklearn.decomposition import PCA\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !nvcc --version\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import cupy\n# print(\"CuPy version:\", cupy.__version__)\n# print(\"CUDA is available:\", cupy.cuda.is_available())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc1,f11,auc1,cm1=train_model(processing_function=pre_processing_PCA)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(acc=acc1, f1=f11, roc_auc=auc1,save_path='/kaggle/working/PCA_eif.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(cm1,save_path='/kaggle/working/confusion_matrix_PCA_eif.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc1if,f11if,auc1if,cm1if=train_model_if(processing_function=pre_processing_if_PCA)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(acc=acc1if, f1=f11if, roc_auc=auc1if,save_path='/kaggle/working/PCA_if.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(cm1if,save_path='/kaggle/working/confusion_matrix_PCA_if.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\ndef pre_processing_smote(data):\n    \n    # Loại bỏ các cột không cần thiết\n    y = data['Label']\n    data.drop(columns=drop_column, inplace=True)\n    data.drop(columns=drop_purposed_column, inplace=True)\n\n    # Tách nhãn mục tiêu và đặc trưng\n    \n    \n\n    # Chuẩn hóa dữ liệu trước khi áp dụng SMOTE\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(data)\n\n    # Áp dụng SMOTE để cân bằng dữ liệu\n    smote = SMOTE(random_state=42)\n    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n\n    return X_resampled","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(train_model(processing_function=pre_processing_smote))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'n_estimators': [100, 200, 500],\n    'max_samples': [256,512],\n    'random_state':[42,50]\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gridSearch(data, params):\n    # Xử lý dữ liệu\n    label = data['Label']\n    label.replace(0, -1, inplace=True)  # Chuyển đổi nhãn để phù hợp với Isolation Forest\n    data = pre_processing_PCA(data)  # Giả sử pre_processing_PCA đã được định nghĩa\n    \n    # Chuẩn hóa dữ liệu\n    scaler = StandardScaler()\n    data_normalized = scaler.fit_transform(data)\n    \n    # Định nghĩa hàm scoring dựa trên accuracy\n    def accuracy_scorer(estimator, X, y):\n        # Dự đoán nhãn (-1 hoặc 1)\n        predictions = estimator.predict(X)  # Không cần gọi fit, vì GridSearchCV đã huấn luyện mô hình\n        return f1_score(y, predictions)\n\n    # Tạo scorer tùy chỉnh\n    custom_scorer = make_scorer(accuracy_scorer, greater_is_better=True)\n    \n    # Grid Search để tìm tham số tối ưu\n    model = IsolationForest()\n    grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=params,\n        scoring=custom_scorer,  # Hàm đánh giá dựa trên độ chính xác\n        cv=3,  # Số lần cross-validation\n        verbose=2,\n        n_jobs=-1  # Sử dụng tất cả CPU\n    )\n    grid_search.fit(data_normalized, label)\n    \n    print(f\"Best Parameters: {grid_search.best_params_}\")\n    print(f\"Best Score: {grid_search.best_score_}\")\n    \n    return grid_search.best_estimator_, grid_search.best_score_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# combined_data = pd.DataFrame()\n# for train_path in train_url[:2]:  # Lấy 15 file đầu tiên\n#     data = pd.read_csv(train_path)\n#     combined_data = pd.concat([combined_data, data], ignore_index=True)\n\n# print(f\"Combined data shape: {combined_data.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# combined_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# best_model,_ = gridSearch(combined_data, params)\n# print(\"Best model:\", best_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Định nghĩa hàm mục tiêu cho Optuna\ndef objective(trial, data, label):\n    # Thử các giá trị tham số ngẫu nhiên\n    n_estimators = trial.suggest_int('n_estimators', 100, 200, 500)\n    contamination = trial.suggest_float('contamination', 0.01, 0.2)\n    max_samples = trial.suggest_float('max_samples', 0.5, 1.0)\n    \n    # Tiền xử lý dữ liệu\n    data_normalized = StandardScaler().fit_transform(data)\n    \n    # Tạo mô hình Isolation Forest\n    model = IsolationForest(n_estimators=n_estimators, contamination=contamination, max_samples=max_samples)\n    model.fit(data_normalized)\n    \n    # Dự đoán nhãn\n    predictions = model.predict(data_normalized)\n    \n    # Chuyển đổi nhãn từ {-1, 1} thành {0, 1} nếu cần thiết\n    predictions = [1 if pred == 1 else 0 for pred in predictions]\n    label = [1 if lbl == 1 else 0 for lbl in label]\n    \n    # Tính F1 Score cho các bài toán phân loại đa lớp (có thể sử dụng 'micro', 'macro', hoặc 'weighted')\n    f1 = f1_score(label, predictions, average='macro')  # Sử dụng 'macro' cho bài toán đa lớp\n    \n    return f1\n\n# Sử dụng Optuna để tối ưu hóa tham số\ndef optimize_with_optuna(data, label):\n    study = optuna.create_study(direction='maximize')  # Tối ưu hóa F1 Score\n    study.optimize(lambda trial: objective(trial, data, label), n_trials=1)  # Thử 50 lần\n    \n    print(\"Best Parameters:\", study.best_params)\n    print(\"Best F1 Score:\", study.best_value)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ouput= combined_data['Label']\n# combined_data = pre_processing(combined_data)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(ouput.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# optimize_with_optuna(combined_data,ouput)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_with_metrics(data, label):\n    # Tạo danh sách các giá trị tham số để thử\n    n_estimators_list = [100, 200, 500]\n    contamination_list = np.linspace(0.1, 0.5, 5)  # Chia 5 giá trị giữa 0.01 và 0.2\n    max_samples_list = np.linspace(0.5, 1.0, 5)  # Chia 5 giá trị giữa 0.5 và 1.0\n    \n    best_f1 = -1  # F1 Score tốt nhất ban đầu\n    best_params = {}  # Tham số tốt nhất\n    best_metrics = {}  # Lưu các chỉ số tốt nhất\n    \n    # Chuẩn hóa dữ liệu\n    data_normalized = StandardScaler().fit_transform(data)\n    \n    # Duyệt qua tất cả các tổ hợp tham số\n    for n_estimators in n_estimators_list:\n        for contamination in contamination_list:\n            for max_samples in max_samples_list:\n                # Tạo mô hình Isolation Forest\n                model = IsolationForest(n_estimators=n_estimators, contamination=contamination, max_samples=max_samples, random_state=42)\n                model.fit(data_normalized)\n                \n                # Dự đoán nhãn\n                predictions = model.predict(data_normalized)\n                \n                # Chuyển đổi nhãn từ {-1, 1} thành {0, 1} nếu cần thiết\n                predictions = [1 if pred == 1 else 0 for pred in predictions]\n                label_converted = [1 if lbl == 1 else 0 for lbl in label]\n                \n                # Tính F1 Score\n                f1 = f1_score(label_converted, predictions, average='macro')\n                \n                # Kiểm tra nếu điểm số tốt hơn, thì lưu lại\n                if f1 > best_f1:\n                    best_f1 = f1\n                    best_params = {\n                        'n_estimators': n_estimators,\n                        'contamination': contamination,\n                        'max_samples': max_samples\n                    }\n                    \n                    # Tính các chỉ số khác\n                    acc = accuracy_score(label_converted, predictions)\n                    try:\n                        auc_roc = roc_auc_score(label_converted, predictions)\n                    except ValueError:\n                        auc_roc = \"N/A (only 2 classes with both present)\"\n                    conf_matrix = confusion_matrix(label_converted, predictions)\n                    \n                    best_metrics = {\n                        'Accuracy': acc,\n                        'AUC-ROC': auc_roc,\n                        'Confusion Matrix': conf_matrix\n                    }\n    \n    # Kết quả\n    print(\"Best Parameters:\", best_params)\n    print(\"Best F1 Score:\", best_f1)\n    print(\"Additional Metrics:\")\n    print(f\"  Accuracy: {best_metrics['Accuracy']}\")\n    print(f\"  AUC-ROC: {best_metrics['AUC-ROC']}\")\n    print(f\"  Confusion Matrix:\\n{best_metrics['Confusion Matrix']}\")\n    \n    return best_params, best_f1, best_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# best_params1, best_f11, best_metrics1= optimize_with_metrics(combined_data,ouput)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_metrics(acc=best_metrics1['Accuracy'], f1=best_f11, roc_auc=best_metrics1['AUC-ROC'],save_path='/kaggle/working/grid.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_confusion_matrix(cm=best_metrics['Confusion Matrix'],save_path='/kaggle/working/confusion_matrix_grid.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}